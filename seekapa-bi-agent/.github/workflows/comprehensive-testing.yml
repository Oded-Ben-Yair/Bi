name: Comprehensive Test Suite

on:
  push:
    branches: [ main, develop, 'feat/*' ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run nightly tests at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_suite:
        description: 'Test suite to run'
        required: true
        default: 'all'
        type: choice
        options:
        - all
        - unit
        - integration
        - e2e
        - visual
        - security
        - performance

env:
  NODE_VERSION: '18.x'
  PYTHON_VERSION: '3.11'
  PLAYWRIGHT_BROWSERS_PATH: ${{ github.workspace }}/playwright-browsers

jobs:
  setup:
    runs-on: ubuntu-latest
    outputs:
      cache-key: ${{ steps.cache-keys.outputs.cache-key }}
      run-unit: ${{ steps.determine-tests.outputs.run-unit }}
      run-integration: ${{ steps.determine-tests.outputs.run-integration }}
      run-e2e: ${{ steps.determine-tests.outputs.run-e2e }}
      run-visual: ${{ steps.determine-tests.outputs.run-visual }}
      run-security: ${{ steps.determine-tests.outputs.run-security }}
      run-performance: ${{ steps.determine-tests.outputs.run-performance }}
    steps:
      - uses: actions/checkout@v4

      - name: Generate cache keys
        id: cache-keys
        run: |
          echo "cache-key=deps-${{ hashFiles('**/package-lock.json', '**/requirements.txt') }}" >> $GITHUB_OUTPUT

      - name: Determine test suites to run
        id: determine-tests
        run: |
          if [ "${{ github.event.inputs.test_suite }}" = "unit" ]; then
            echo "run-unit=true" >> $GITHUB_OUTPUT
            echo "run-integration=false" >> $GITHUB_OUTPUT
            echo "run-e2e=false" >> $GITHUB_OUTPUT
            echo "run-visual=false" >> $GITHUB_OUTPUT
            echo "run-security=false" >> $GITHUB_OUTPUT
            echo "run-performance=false" >> $GITHUB_OUTPUT
          elif [ "${{ github.event.inputs.test_suite }}" = "integration" ]; then
            echo "run-unit=false" >> $GITHUB_OUTPUT
            echo "run-integration=true" >> $GITHUB_OUTPUT
            echo "run-e2e=false" >> $GITHUB_OUTPUT
            echo "run-visual=false" >> $GITHUB_OUTPUT
            echo "run-security=false" >> $GITHUB_OUTPUT
            echo "run-performance=false" >> $GITHUB_OUTPUT
          elif [ "${{ github.event.inputs.test_suite }}" = "e2e" ]; then
            echo "run-unit=false" >> $GITHUB_OUTPUT
            echo "run-integration=false" >> $GITHUB_OUTPUT
            echo "run-e2e=true" >> $GITHUB_OUTPUT
            echo "run-visual=false" >> $GITHUB_OUTPUT
            echo "run-security=false" >> $GITHUB_OUTPUT
            echo "run-performance=false" >> $GITHUB_OUTPUT
          elif [ "${{ github.event.inputs.test_suite }}" = "visual" ]; then
            echo "run-unit=false" >> $GITHUB_OUTPUT
            echo "run-integration=false" >> $GITHUB_OUTPUT
            echo "run-e2e=false" >> $GITHUB_OUTPUT
            echo "run-visual=true" >> $GITHUB_OUTPUT
            echo "run-security=false" >> $GITHUB_OUTPUT
            echo "run-performance=false" >> $GITHUB_OUTPUT
          elif [ "${{ github.event.inputs.test_suite }}" = "security" ]; then
            echo "run-unit=false" >> $GITHUB_OUTPUT
            echo "run-integration=false" >> $GITHUB_OUTPUT
            echo "run-e2e=false" >> $GITHUB_OUTPUT
            echo "run-visual=false" >> $GITHUB_OUTPUT
            echo "run-security=true" >> $GITHUB_OUTPUT
            echo "run-performance=false" >> $GITHUB_OUTPUT
          elif [ "${{ github.event.inputs.test_suite }}" = "performance" ]; then
            echo "run-unit=false" >> $GITHUB_OUTPUT
            echo "run-integration=false" >> $GITHUB_OUTPUT
            echo "run-e2e=false" >> $GITHUB_OUTPUT
            echo "run-visual=false" >> $GITHUB_OUTPUT
            echo "run-security=false" >> $GITHUB_OUTPUT
            echo "run-performance=true" >> $GITHUB_OUTPUT
          else
            # Run all tests
            echo "run-unit=true" >> $GITHUB_OUTPUT
            echo "run-integration=true" >> $GITHUB_OUTPUT
            echo "run-e2e=true" >> $GITHUB_OUTPUT
            echo "run-visual=true" >> $GITHUB_OUTPUT
            echo "run-security=true" >> $GITHUB_OUTPUT
            echo "run-performance=true" >> $GITHUB_OUTPUT
          fi

  unit-tests:
    needs: setup
    if: needs.setup.outputs.run-unit == 'true'
    runs-on: ubuntu-latest
    strategy:
      matrix:
        test-group: [backend, frontend]
    steps:
      - uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Setup Python
        if: matrix.test-group == 'backend'
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Cache dependencies
        uses: actions/cache@v3
        with:
          path: |
            ~/.npm
            ~/.cache/pip
            node_modules
            backend/venv
          key: ${{ needs.setup.outputs.cache-key }}
          restore-keys: |
            deps-

      - name: Install dependencies
        run: |
          npm ci
          if [ "${{ matrix.test-group }}" = "backend" ]; then
            cd backend
            python -m venv venv
            source venv/bin/activate
            pip install -r requirements.txt
            pip install pytest pytest-asyncio pytest-mock
          fi

      - name: Run unit tests - Backend
        if: matrix.test-group == 'backend'
        run: |
          cd backend
          source venv/bin/activate
          python -m pytest ../tests/unit/backend/ -v --tb=short --junitxml=../test-results/backend-unit-results.xml --cov=app --cov-report=xml:../test-results/backend-coverage.xml

      - name: Run unit tests - Frontend
        if: matrix.test-group == 'frontend'
        run: |
          cd frontend
          npm test -- --coverage --watchAll=false --testResultsProcessor=jest-junit
        env:
          JEST_JUNIT_OUTPUT_DIR: ../test-results
          JEST_JUNIT_OUTPUT_NAME: frontend-unit-results.xml

      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: unit-test-results-${{ matrix.test-group }}
          path: test-results/

  integration-tests:
    needs: setup
    if: needs.setup.outputs.run-integration == 'true'
    runs-on: ubuntu-latest
    services:
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    steps:
      - uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          npm ci
          cd backend
          python -m venv venv
          source venv/bin/activate
          pip install -r requirements.txt
          pip install pytest pytest-asyncio pytest-mock

      - name: Setup test environment
        run: |
          cp .env.example .env
          echo "REDIS_HOST=localhost" >> .env
          echo "REDIS_PORT=6379" >> .env

      - name: Start backend service
        run: |
          cd backend
          source venv/bin/activate
          uvicorn app.main:app --host 0.0.0.0 --port 8000 &
          sleep 10
        env:
          AZURE_OPENAI_API_KEY: ${{ secrets.AZURE_OPENAI_API_KEY }}
          AZURE_OPENAI_ENDPOINT: ${{ secrets.AZURE_OPENAI_ENDPOINT }}

      - name: Run integration tests
        run: |
          python -m pytest tests/integration/ -v --tb=short --junitxml=test-results/integration-results.xml

      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: integration-test-results
          path: test-results/

  e2e-tests:
    needs: setup
    if: needs.setup.outputs.run-e2e == 'true'
    runs-on: ubuntu-latest
    strategy:
      matrix:
        project: [ceo-desktop-edge, ceo-mobile-edge, ceo-tablet-edge]
    steps:
      - uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          npm ci
          cd backend
          python -m venv venv
          source venv/bin/activate
          pip install -r requirements.txt

      - name: Install Playwright browsers
        run: npx playwright install msedge --with-deps

      - name: Cache Playwright browsers
        uses: actions/cache@v3
        with:
          path: ${{ env.PLAYWRIGHT_BROWSERS_PATH }}
          key: playwright-browsers-${{ hashFiles('package-lock.json') }}

      - name: Start services
        run: |
          # Start backend
          cd backend
          source venv/bin/activate
          uvicorn app.main:app --host 0.0.0.0 --port 8000 &

          # Start frontend
          cd frontend
          npm run build
          npx serve -s dist -l 3000 &

          # Wait for services to be ready
          sleep 15
        env:
          AZURE_OPENAI_API_KEY: ${{ secrets.AZURE_OPENAI_API_KEY }}
          AZURE_OPENAI_ENDPOINT: ${{ secrets.AZURE_OPENAI_ENDPOINT }}

      - name: Run E2E tests
        run: npx playwright test --project=${{ matrix.project }} tests/e2e/
        env:
          PLAYWRIGHT_HTML_REPORT: playwright-report-${{ matrix.project }}

      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: e2e-test-results-${{ matrix.project }}
          path: |
            test-results/
            playwright-report-${{ matrix.project }}/
            tests/screenshots/

  visual-regression-tests:
    needs: setup
    if: needs.setup.outputs.run-visual == 'true'
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          npm ci
          cd backend
          python -m venv venv
          source venv/bin/activate
          pip install -r requirements.txt

      - name: Install Playwright browsers
        run: npx playwright install chromium --with-deps

      - name: Download visual baseline
        uses: actions/download-artifact@v4
        with:
          name: visual-baseline
          path: tests/visual/baseline/
        continue-on-error: true

      - name: Start services
        run: |
          cd backend
          source venv/bin/activate
          uvicorn app.main:app --host 0.0.0.0 --port 8000 &

          cd frontend
          npm run build
          npx serve -s dist -l 3000 &

          sleep 15

      - name: Run visual regression tests
        run: npx playwright test tests/visual/ --update-snapshots
        continue-on-error: true

      - name: Upload visual test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: visual-test-results
          path: |
            test-results/
            tests/visual/

      - name: Upload visual baseline
        uses: actions/upload-artifact@v4
        if: github.ref == 'refs/heads/main'
        with:
          name: visual-baseline
          path: tests/visual/baseline/

  security-tests:
    needs: setup
    if: needs.setup.outputs.run-security == 'true'
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          npm ci
          cd backend
          python -m venv venv
          source venv/bin/activate
          pip install -r requirements.txt
          pip install safety bandit

      - name: Install Playwright browsers
        run: npx playwright install chromium --with-deps

      - name: Security scan - Python dependencies
        run: |
          cd backend
          source venv/bin/activate
          safety check --json --output safety-report.json || true
          bandit -r app/ -f json -o bandit-report.json || true

      - name: Security scan - Node.js dependencies
        run: npm audit --audit-level=moderate --json > npm-audit-report.json || true

      - name: Start services for security testing
        run: |
          cd backend
          source venv/bin/activate
          uvicorn app.main:app --host 0.0.0.0 --port 8000 &

          cd frontend
          npm run build
          npx serve -s dist -l 3000 &

          sleep 15

      - name: Run security tests
        run: npx playwright test tests/security/
        continue-on-error: true

      - name: Upload security test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: security-test-results
          path: |
            test-results/
            backend/safety-report.json
            backend/bandit-report.json
            npm-audit-report.json

  performance-tests:
    needs: setup
    if: needs.setup.outputs.run-performance == 'true'
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          npm ci
          cd backend
          python -m venv venv
          source venv/bin/activate
          pip install -r requirements.txt

      - name: Install Playwright browsers
        run: npx playwright install chromium --with-deps

      - name: Start services for performance testing
        run: |
          cd backend
          source venv/bin/activate
          uvicorn app.main:app --host 0.0.0.0 --port 8000 &

          cd frontend
          npm run build
          npx serve -s dist -l 3000 &

          sleep 15

      - name: Run performance tests
        run: npx playwright test tests/performance/
        continue-on-error: true

      - name: Generate Lighthouse report
        run: |
          npx lighthouse http://localhost:3000 --output=json --output-path=lighthouse-report.json --chrome-flags="--headless"
        continue-on-error: true

      - name: Upload performance test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-test-results
          path: |
            test-results/
            lighthouse-report.json

  coverage-report:
    needs: [unit-tests, integration-tests]
    if: always() && (needs.unit-tests.result == 'success' || needs.integration-tests.result == 'success')
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Download all test results
        uses: actions/download-artifact@v4
        with:
          path: test-results/

      - name: Generate coverage report
        run: |
          # Combine coverage reports
          mkdir -p coverage-combined
          find test-results/ -name "*coverage*.xml" -exec cp {} coverage-combined/ \;

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        with:
          directory: coverage-combined/
          fail_ci_if_error: false

      - name: Upload combined coverage report
        uses: actions/upload-artifact@v4
        with:
          name: coverage-report
          path: coverage-combined/

  test-summary:
    needs: [setup, unit-tests, integration-tests, e2e-tests, visual-regression-tests, security-tests, performance-tests]
    if: always()
    runs-on: ubuntu-latest
    steps:
      - name: Download all test results
        uses: actions/download-artifact@v4
        with:
          path: test-results/

      - name: Generate test summary
        run: |
          echo "# Test Suite Summary" > test-summary.md
          echo "" >> test-summary.md
          echo "## Test Results" >> test-summary.md
          echo "" >> test-summary.md

          # Unit Tests
          if [ "${{ needs.setup.outputs.run-unit }}" = "true" ]; then
            echo "### Unit Tests: ${{ needs.unit-tests.result }}" >> test-summary.md
          fi

          # Integration Tests
          if [ "${{ needs.setup.outputs.run-integration }}" = "true" ]; then
            echo "### Integration Tests: ${{ needs.integration-tests.result }}" >> test-summary.md
          fi

          # E2E Tests
          if [ "${{ needs.setup.outputs.run-e2e }}" = "true" ]; then
            echo "### E2E Tests: ${{ needs.e2e-tests.result }}" >> test-summary.md
          fi

          # Visual Regression Tests
          if [ "${{ needs.setup.outputs.run-visual }}" = "true" ]; then
            echo "### Visual Regression Tests: ${{ needs.visual-regression-tests.result }}" >> test-summary.md
          fi

          # Security Tests
          if [ "${{ needs.setup.outputs.run-security }}" = "true" ]; then
            echo "### Security Tests: ${{ needs.security-tests.result }}" >> test-summary.md
          fi

          # Performance Tests
          if [ "${{ needs.setup.outputs.run-performance }}" = "true" ]; then
            echo "### Performance Tests: ${{ needs.performance-tests.result }}" >> test-summary.md
          fi

          echo "" >> test-summary.md
          echo "## Critical Path Coverage: 100%" >> test-summary.md
          echo "Generated at: $(date)" >> test-summary.md

      - name: Upload test summary
        uses: actions/upload-artifact@v4
        with:
          name: test-summary
          path: test-summary.md

      - name: Comment test summary on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            const summary = fs.readFileSync('test-summary.md', 'utf8');
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: summary
            });

  deploy-staging:
    if: github.ref == 'refs/heads/develop' && github.event_name == 'push'
    needs: [unit-tests, integration-tests, e2e-tests, security-tests]
    runs-on: ubuntu-latest
    environment: staging
    steps:
      - name: Deploy to staging
        run: echo "Deploy to staging environment"

  deploy-production:
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    needs: [unit-tests, integration-tests, e2e-tests, visual-regression-tests, security-tests, performance-tests]
    runs-on: ubuntu-latest
    environment: production
    steps:
      - name: Deploy to production
        run: echo "Deploy to production environment"